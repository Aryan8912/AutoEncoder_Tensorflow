{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZbrYMN0Jv7YS8kvhNdhw4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan8912/AutoEncoder_Tensorflow/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensor2tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTOQ7Rh4gUYv",
        "outputId": "094b9ce1-ea9f-421a-e16d-43996b2f97a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.4.0)\n",
            "Collecting bz2file (from tensor2tensor)\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (4.1.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (3.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.0.0)\n",
            "Collecting gevent (from tensor2tensor)\n",
            "  Downloading gevent-24.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (2.155.0)\n",
            "Collecting gunicorn (from tensor2tensor)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (0.25.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (3.12.1)\n",
            "Collecting kfac (from tensor2tensor)\n",
            "  Downloading kfac-0.2.4-py2.py3-none-any.whl.metadata (992 bytes)\n",
            "Collecting mesh-tensorflow (from tensor2tensor)\n",
            "  Downloading mesh_tensorflow-0.1.21-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.26.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (4.1.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (11.1.0)\n",
            "Collecting pypng (from tensor2tensor)\n",
            "  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.13.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.17.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.13.1)\n",
            "Collecting tensorflow-addons (from tensor2tensor)\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (4.9.7)\n",
            "Collecting tensorflow-gan (from tensor2tensor)\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tensorflow-probability==0.7.0 (from tensor2tensor)\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensor2tensor) (4.67.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (3.1.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (2.17.1)\n",
            "Requirement already satisfied: flax>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (0.10.2)\n",
            "Requirement already satisfied: jax>=0.1.72 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.51 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (0.4.33)\n",
            "Requirement already satisfied: pygame>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (2.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl->tensor2tensor) (2.2.2)\n",
            "INFO: pip is looking at multiple versions of dopamine-rl to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting dopamine-rl (from tensor2tensor)\n",
            "  Downloading dopamine_rl-4.1.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ale-py>=0.10.1 (from dopamine-rl->tensor2tensor)\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting gymnasium>=1.0.0 (from dopamine-rl->tensor2tensor)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting python-snappy>=0.7.3 (from dopamine-rl->tensor2tensor)\n",
            "  Downloading python_snappy-0.7.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting dopamine-rl (from tensor2tensor)\n",
            "  Downloading dopamine_rl-4.1.1-py3-none-any.whl.metadata (8.2 kB)\n",
            "  Downloading dopamine_rl-4.0.9-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.8-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.7-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.5-py3-none-any.whl.metadata (8.1 kB)\n",
            "INFO: pip is still looking at multiple versions of dopamine-rl to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dopamine_rl-4.0.2-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-4.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "  Downloading dopamine_rl-3.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->tensor2tensor) (0.0.8)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask->tensor2tensor) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask->tensor2tensor) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask->tensor2tensor) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask->tensor2tensor) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask->tensor2tensor) (1.9.0)\n",
            "Collecting zope.event (from gevent->tensor2tensor)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting zope.interface (from gevent->tensor2tensor)\n",
            "  Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gevent->tensor2tensor) (3.1.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->tensor2tensor) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->tensor2tensor) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->tensor2tensor) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->tensor2tensor) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->tensor2tensor) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn->tensor2tensor) (24.2)\n",
            "INFO: pip is looking at multiple versions of kfac to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting kfac (from tensor2tensor)\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl.metadata (919 bytes)\n",
            "  Downloading kfac-0.2.2-py2.py3-none-any.whl.metadata (912 bytes)\n",
            "  Downloading kfac-0.2.0-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tensor2tensor) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tensor2tensor) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tensor2tensor) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tensor2tensor) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tensor2tensor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tensor2tensor) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tensor2tensor) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->tensor2tensor) (1.3.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->tensor2tensor)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (4.25.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (1.17.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tensor2tensor) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tensor2tensor) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gan->tensor2tensor) (0.16.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tensor2tensor) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tensor2tensor) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tensor2tensor) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tensor2tensor) (3.21.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.71)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (6.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client->tensor2tensor) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client->tensor2tensor) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client->tensor2tensor) (5.5.0)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari]>=0.13.1->dopamine-rl->tensor2tensor)\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->tensor2tensor) (3.2.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.72->dopamine-rl->tensor2tensor) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.72->dopamine-rl->tensor2tensor) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask->tensor2tensor) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2024.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (18.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (75.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.37.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.2->tensorflow-gan->tensor2tensor) (2.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tensor2tensor) (0.16)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.45.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (2.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.7.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.88)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl->tensor2tensor) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl->tensor2tensor) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.2)\n",
            "Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.4/981.4 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dopamine_rl-3.2.1-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gevent-24.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: bz2file\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6868 sha256=fc242f7ead406f196915a69d23989bf57b9979a84c71d02ec68636456641f8a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/ee/f7/6fccd10cb65421ba2da64fa6caf8ee7fbae0059884af8c8587\n",
            "Successfully built bz2file\n",
            "Installing collected packages: pypng, bz2file, zope.interface, zope.event, typeguard, tensorflow-probability, mesh-tensorflow, gunicorn, ale-py, tensorflow-addons, kfac, gevent, tensorflow-gan, dopamine-rl, tensor2tensor\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.1\n",
            "    Uninstalling typeguard-4.4.1:\n",
            "      Successfully uninstalled typeguard-4.4.1\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: dopamine-rl\n",
            "    Found existing installation: dopamine_rl 4.1.0\n",
            "    Uninstalling dopamine_rl-4.1.0:\n",
            "      Successfully uninstalled dopamine_rl-4.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.7.5 bz2file-0.98 dopamine-rl-3.2.1 gevent-24.11.1 gunicorn-23.0.0 kfac-0.2.0 mesh-tensorflow-0.1.21 pypng-0.20220715.0 tensor2tensor-1.15.7 tensorflow-addons-0.23.0 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 typeguard-2.13.3 zope.event-5.0 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "zM17oNHyOqBX",
        "outputId": "f38cf49c-daa0-487f-e81b-8ceaac370053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.17.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensor2tensor/utils/contrib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_slim\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mis_tf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1082443ff9d9>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_hparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensor2tensor/layers/common_attention.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marea_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensor2tensor/layers/area_attention.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensor2tensor/layers/common_layers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor2tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensor2tensor/utils/contrib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Some features are now available in separate packages. We shim support for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# these as needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mis_tf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2023 The Tensor2Tensor Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Autoencoders.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensor2tensor.layers import common_attention\n",
        "from tensor2tensor.layers import common_hparams\n",
        "from tensor2tensor.layers import common_layers\n",
        "from tensor2tensor.layers import discretization\n",
        "from tensor2tensor.layers import latent_layers\n",
        "from tensor2tensor.layers import modalities\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import t2t_model\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor import layers\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.compat.v1 import estimator as tf_estimator\n",
        "\n",
        "\n",
        "def reverse_gradient(x, lr=1.0):\n",
        "  return -lr * x + tf.stop_gradient((1.0 + lr) * x)\n",
        "\n",
        "\n",
        "def time_to_channels(embedded_video):\n",
        "  \"\"\"Put time dimension on channels in an embedded video.\"\"\"\n",
        "  video_shape = common_layers.shape_list(embedded_video)\n",
        "  if len(video_shape) != 5:\n",
        "    raise ValueError(\"Assuming videos given as tensors in the format \"\n",
        "                     \"[batch, time, height, width, channels] but got one \"\n",
        "                     \"of shape: %s\" % str(video_shape))\n",
        "  transposed = tf.transpose(embedded_video, [0, 2, 3, 1, 4])\n",
        "  return tf.reshape(transposed, [\n",
        "      video_shape[0], video_shape[2], video_shape[3],\n",
        "      video_shape[1] * video_shape[4]\n",
        "  ])\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderBasic(t2t_model.T2TModel):\n",
        "  \"\"\"A basic autoencoder, try with image_mnist_rev or image_cifar10_rev.\"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(AutoencoderBasic, self).__init__(*args, **kwargs)\n",
        "    self._cur_bottleneck_tensor = None\n",
        "    self.is1d = None\n",
        "    self._encode_on_predict = False\n",
        "\n",
        "  @property\n",
        "  def num_channels(self):\n",
        "    # TODO(lukaszkaiser): is this a universal enough way to get channels?\n",
        "    try:\n",
        "      num_channels = self.hparams.problem.num_channels\n",
        "    except AttributeError:\n",
        "      num_channels = 1\n",
        "    return num_channels\n",
        "\n",
        "  def image_summary(self, name, image_logits, max_outputs=1):\n",
        "    \"\"\"Helper for image summaries that are safe on TPU.\"\"\"\n",
        "    if len(image_logits.get_shape()) != 5:\n",
        "      tf.logging.info(\"Not generating image summary, maybe not an image.\")\n",
        "      return\n",
        "    return tf.summary.image(\n",
        "        name,\n",
        "        common_layers.tpu_safe_image_summary(tf.argmax(image_logits, -1)),\n",
        "        max_outputs=max_outputs)\n",
        "\n",
        "  def embed(self, x, name=\"embedding\"):\n",
        "    \"\"\"Input embedding with a non-zero bias for uniform inputs.\"\"\"\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "      x_shape = common_layers.shape_list(x)\n",
        "      # Merge channels and depth before embedding.\n",
        "      x = tf.reshape(x, x_shape[:-2] + [x_shape[-2] * x_shape[-1]])\n",
        "      x = tf.layers.dense(\n",
        "          x,\n",
        "          self.hparams.hidden_size,\n",
        "          name=\"embed\",\n",
        "          activation=common_layers.belu,\n",
        "          bias_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "      x = common_layers.layer_norm(x, name=\"ln_embed\")\n",
        "      return common_attention.add_timing_signal_nd(x)\n",
        "\n",
        "  def bottleneck(self, x):\n",
        "    with tf.variable_scope(\"bottleneck\"):\n",
        "      hparams = self.hparams\n",
        "      x = tf.layers.dense(x, hparams.bottleneck_bits, name=\"bottleneck\")\n",
        "      if hparams.mode == tf_estimator.ModeKeys.TRAIN:\n",
        "        noise = 2.0 * tf.random_uniform(common_layers.shape_list(x)) - 1.0\n",
        "        return tf.tanh(x) + noise * hparams.bottleneck_noise, 0.0\n",
        "      return tf.tanh(x), 0.0\n",
        "\n",
        "  def unbottleneck(self, x, res_size, reuse=None):\n",
        "    with tf.variable_scope(\"unbottleneck\", reuse=reuse):\n",
        "      x = tf.layers.dense(x, res_size, name=\"dense\")\n",
        "      return x\n",
        "\n",
        "  def make_even_size(self, x):\n",
        "    if not self.is1d:\n",
        "      return common_layers.make_even_size(x)\n",
        "    shape1 = x.get_shape().as_list()[1]\n",
        "    if shape1 is not None and shape1 % 2 == 0:\n",
        "      return x\n",
        "    x, _ = common_layers.pad_to_same_length(\n",
        "        x, x, final_length_divisible_by=2, axis=1)\n",
        "    return x\n",
        "\n",
        "  def encoder(self, x):\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "      hparams = self.hparams\n",
        "      layers = []\n",
        "      kernel, strides = self._get_kernel_and_strides()\n",
        "      # Down-convolutions.\n",
        "      for i in range(hparams.num_hidden_layers):\n",
        "        x = self.make_even_size(x)\n",
        "        layers.append(x)\n",
        "        x = tf.layers.conv2d(\n",
        "            x,\n",
        "            hparams.hidden_size * 2**(i + 1),\n",
        "            kernel,\n",
        "            strides=strides,\n",
        "            padding=\"SAME\",\n",
        "            activation=common_layers.belu,\n",
        "            name=\"conv_%d\" % i)\n",
        "        x = common_layers.layer_norm(x, name=\"ln_%d\" % i)\n",
        "      return x, layers\n",
        "\n",
        "  def decoder(self, x, encoder_layers):\n",
        "    del encoder_layers\n",
        "    with tf.variable_scope(\"decoder\"):\n",
        "      hparams = self.hparams\n",
        "      kernel, strides = self._get_kernel_and_strides()\n",
        "      # Up-convolutions.\n",
        "      for i in range(hparams.num_hidden_layers):\n",
        "        j = hparams.num_hidden_layers - i - 1\n",
        "        x = tf.layers.conv2d_transpose(\n",
        "            x,\n",
        "            hparams.hidden_size * 2**j,\n",
        "            kernel,\n",
        "            strides=strides,\n",
        "            padding=\"SAME\",\n",
        "            activation=common_layers.belu,\n",
        "            name=\"deconv_%d\" % j)\n",
        "        x = common_layers.layer_norm(x, name=\"ln_%d\" % i)\n",
        "      return x\n",
        "\n",
        "  def gumbel_sample(self, reconstr_gan):\n",
        "    hparams = self.hparams\n",
        "    is_training = hparams.mode == tf_estimator.ModeKeys.TRAIN\n",
        "    vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
        "    if hasattr(self._hparams, \"vocab_divisor\"):\n",
        "      vocab_size += (-vocab_size) % self._hparams.vocab_divisor\n",
        "    reconstr_gan = tf.nn.log_softmax(reconstr_gan)\n",
        "    if is_training and hparams.gumbel_temperature > 0.0:\n",
        "      gumbel_samples = discretization.gumbel_sample(\n",
        "          common_layers.shape_list(reconstr_gan))\n",
        "      gumbel_samples *= hparams.gumbel_noise_factor\n",
        "      reconstr_gan += gumbel_samples\n",
        "      reconstr_sample = latent_layers.multinomial_sample(\n",
        "          reconstr_gan, temperature=hparams.gumbel_temperature)\n",
        "      reconstr_gan = tf.nn.softmax(reconstr_gan / hparams.gumbel_temperature)\n",
        "    else:\n",
        "      reconstr_sample = tf.argmax(reconstr_gan, axis=-1)\n",
        "      reconstr_gan = tf.nn.softmax(reconstr_gan / 0.1)  # Sharpen a bit.\n",
        "    # Use 1-hot forward, softmax backward.\n",
        "    reconstr_hot = tf.one_hot(reconstr_sample, vocab_size)\n",
        "    reconstr_gan += reconstr_hot - tf.stop_gradient(reconstr_gan)\n",
        "    return reconstr_gan\n",
        "\n",
        "  def body(self, features):\n",
        "    hparams = self.hparams\n",
        "    is_training = hparams.mode == tf_estimator.ModeKeys.TRAIN\n",
        "    vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
        "    if hasattr(self._hparams, \"vocab_divisor\"):\n",
        "      vocab_size += (-vocab_size) % self._hparams.vocab_divisor\n",
        "    encoder_layers = None\n",
        "    self.is1d = hparams.sample_width == 1\n",
        "    if (hparams.mode != tf_estimator.ModeKeys.PREDICT\n",
        "        or self._encode_on_predict):\n",
        "      labels = features[\"targets_raw\"]\n",
        "      labels_shape = common_layers.shape_list(labels)\n",
        "      # handle videos\n",
        "      if len(labels.shape) == 5:\n",
        "        labels = time_to_channels(labels)\n",
        "      shape = common_layers.shape_list(labels)\n",
        "      x = tf.one_hot(labels, vocab_size)\n",
        "      x = self.embed(x)\n",
        "      target_codes = x\n",
        "      if shape[2] == 1:\n",
        "        self.is1d = True\n",
        "      # Run encoder.\n",
        "      x, encoder_layers = self.encoder(x)\n",
        "      # Bottleneck.\n",
        "      b, b_loss = self.bottleneck(x)\n",
        "      xb_loss = 0.0\n",
        "      b_shape = common_layers.shape_list(b)\n",
        "      self._cur_bottleneck_tensor = b\n",
        "      res_size = common_layers.shape_list(x)[-1]\n",
        "      b = self.unbottleneck(b, res_size)\n",
        "      if not is_training:\n",
        "        x = b\n",
        "      else:\n",
        "        l = 2**hparams.num_hidden_layers\n",
        "        warm_step = int(hparams.bottleneck_warmup_steps * 0.25 * l)\n",
        "        nomix_p = common_layers.inverse_lin_decay(warm_step) + 0.01\n",
        "        if common_layers.should_generate_summaries():\n",
        "          tf.summary.scalar(\"nomix_p_bottleneck\", nomix_p)\n",
        "        rand = tf.random_uniform(common_layers.shape_list(x))\n",
        "        # This is the distance between b and x. Having this as loss helps learn\n",
        "        # the bottleneck function, but if we back-propagated to x it would be\n",
        "        # minimized by just setting x=0 and b=0 -- so we don't want too much\n",
        "        # of the influence of this, and we stop-gradient to not zero-out x.\n",
        "        x_stop = tf.stop_gradient(x)\n",
        "        xb_loss = tf.reduce_mean(tf.reduce_sum(\n",
        "            tf.squared_difference(x_stop, b), axis=-1))\n",
        "        # To prevent this loss from exploding we clip at 1, but anneal clipping.\n",
        "        clip_max = 1.0 / common_layers.inverse_exp_decay(\n",
        "            warm_step, min_value=0.001)\n",
        "        xb_clip = tf.maximum(tf.stop_gradient(xb_loss), clip_max)\n",
        "        xb_loss *= clip_max / xb_clip\n",
        "        x = tf.where(tf.less(rand, nomix_p), b, x)\n",
        "      if hparams.gan_loss_factor != 0.0:\n",
        "        # Add a purely sampled batch on which we'll compute the GAN loss.\n",
        "        g = self.unbottleneck(\n",
        "            self.sample(shape=b_shape),\n",
        "            common_layers.shape_list(x)[-1],\n",
        "            reuse=True)\n",
        "        x = tf.concat([x, g], axis=0)\n",
        "    else:\n",
        "      if self._cur_bottleneck_tensor is None:\n",
        "        b = self.sample()\n",
        "      else:\n",
        "        b = self._cur_bottleneck_tensor\n",
        "      self._cur_bottleneck_tensor = b\n",
        "      res_size = self.hparams.hidden_size * 2**self.hparams.num_hidden_layers\n",
        "      res_size = min(res_size, hparams.max_hidden_size)\n",
        "      x = self.unbottleneck(b, res_size)\n",
        "    # Run decoder.\n",
        "    x = self.decoder(x, encoder_layers)\n",
        "\n",
        "    # Cut to the right size and mix before returning.\n",
        "    res = x\n",
        "    if hparams.mode != tf_estimator.ModeKeys.PREDICT:\n",
        "      res = x[:, :shape[1], :shape[2], :]\n",
        "\n",
        "    # Final dense layer.\n",
        "    res = tf.layers.dense(\n",
        "        res, self.num_channels * hparams.hidden_size, name=\"res_dense\")\n",
        "\n",
        "    output_shape = common_layers.shape_list(res)[:-1] + [\n",
        "        self.num_channels, self.hparams.hidden_size\n",
        "    ]\n",
        "    res = tf.reshape(res, output_shape)\n",
        "\n",
        "    if hparams.mode == tf_estimator.ModeKeys.PREDICT:\n",
        "      if hparams.use_vq_loss:\n",
        "        (reconstr, _, _, _, _) = discretization.vq_loss(res, labels, vocab_size)\n",
        "      else:\n",
        "        reconstr = tf.layers.dense(res, vocab_size, name=\"autoencoder_final\")\n",
        "      return reconstr, {\"bottleneck_loss\": 0.0}\n",
        "\n",
        "    if hparams.gan_loss_factor != 0.0:\n",
        "      res, res_gan = tf.split(res, 2, axis=0)\n",
        "\n",
        "    # Losses.\n",
        "    losses = {\n",
        "        \"bottleneck_extra\": b_loss,\n",
        "        \"bottleneck_l2\": hparams.bottleneck_l2_factor * xb_loss\n",
        "    }\n",
        "\n",
        "    if hparams.use_vq_loss:\n",
        "      vq_temperature = hparams.vq_temperature / common_layers.inverse_exp_decay(\n",
        "          hparams.gan_codes_warmup_steps * 1.2,\n",
        "          min_value=hparams.vq_temperature * 2)\n",
        "      if hparams.mode != tf_estimator.ModeKeys.TRAIN:\n",
        "        vq_temperature = None\n",
        "      with tf.variable_scope(\"vq_loss\"):\n",
        "        (reconstr, _, target_codes, code_loss,\n",
        "         targets_loss) = discretization.vq_loss(\n",
        "             res, labels, vocab_size, temperature=vq_temperature)\n",
        "      losses[\"code_loss\"] = code_loss * hparams.code_loss_factor\n",
        "      losses[\"training\"] = targets_loss\n",
        "    else:\n",
        "      reconstr = tf.layers.dense(res, vocab_size, name=\"autoencoder_final\")\n",
        "      targets_loss = tf.losses.sparse_softmax_cross_entropy(\n",
        "          logits=tf.reshape(reconstr, labels_shape + [vocab_size]),\n",
        "          labels=tf.reshape(labels, labels_shape))\n",
        "      losses[\"training\"] = targets_loss\n",
        "\n",
        "    # GAN losses.\n",
        "    if hparams.gan_loss_factor != 0.0:\n",
        "      update_means_factor = common_layers.inverse_exp_decay(\n",
        "          hparams.gan_codes_warmup_steps, min_value=0.0001)\n",
        "      if hparams.use_vq_loss:\n",
        "        with tf.variable_scope(\"vq_loss\", reuse=True):\n",
        "          update_means = tf.less(tf.random_uniform([]), update_means_factor)\n",
        "          reconstr_gan, gan_codes, _, code_loss_gan, _ = discretization.vq_loss(\n",
        "              res_gan,\n",
        "              labels,\n",
        "              vocab_size,\n",
        "              do_update=update_means,\n",
        "              temperature=vq_temperature)\n",
        "          reconstr_gan_nonoise = reconstr_gan\n",
        "          code_loss_gan *= hparams.code_loss_factor * update_means_factor\n",
        "          losses[\"code_loss_gan\"] = code_loss_gan\n",
        "      else:\n",
        "        reconstr_gan = tf.layers.dense(\n",
        "            res_gan, vocab_size, name=\"autoencoder_final\", reuse=True)\n",
        "        reconstr_gan_nonoise = reconstr_gan\n",
        "        reconstr_gan = self.gumbel_sample(reconstr_gan)\n",
        "        # Embed to codes.\n",
        "        gan_codes = self.embed(reconstr_gan)\n",
        "\n",
        "    # Add GAN loss if requested.\n",
        "    gan_loss = 0.0\n",
        "    if hparams.gan_loss_factor != 0.0:\n",
        "      self.image_summary(\"gan\", reconstr_gan_nonoise)\n",
        "\n",
        "      def discriminate(x):\n",
        "        \"\"\"Run a dioscriminator depending on the hparams.\"\"\"\n",
        "        if hparams.discriminator == \"default\":\n",
        "          return common_layers.deep_discriminator(\n",
        "              x, hparams.discriminator_batchnorm, is_training)\n",
        "        elif hparams.discriminator == \"patched\":\n",
        "          return common_layers.patch_discriminator(x)\n",
        "        elif hparams.discriminator == \"single\":\n",
        "          return common_layers.single_discriminator(\n",
        "              x,\n",
        "              hparams.discriminator_size,\n",
        "              hparams.discriminator_kernel_size,\n",
        "              hparams.discriminator_strides,\n",
        "              pure_mean=hparams.discriminator_pure_mean)\n",
        "        elif hparams.discriminator == \"double\":\n",
        "          return common_layers.double_discriminator(\n",
        "              x,\n",
        "              hparams.discriminator_size,\n",
        "              hparams.discriminator_kernel_size,\n",
        "              hparams.discriminator_strides,\n",
        "              pure_mean=hparams.discriminator_pure_mean)\n",
        "        else:\n",
        "          raise Exception(\"Unknown discriminator %s\" % hparams.discriminator)\n",
        "\n",
        "      tc_shape = common_layers.shape_list(target_codes)\n",
        "      if len(tc_shape) > 4:\n",
        "        target_codes = tf.reshape(target_codes,\n",
        "                                  tc_shape[:-2] + [tc_shape[-1] * tc_shape[-2]])\n",
        "        gan_codes = tf.reshape(gan_codes,\n",
        "                               tc_shape[:-2] + [tc_shape[-1] * tc_shape[-2]])\n",
        "      gan_lr = common_layers.inverse_exp_decay(\n",
        "          hparams.gan_codes_warmup_steps * 1.5)\n",
        "      rev_grad_gan_codes = reverse_gradient(gan_codes, lr=gan_lr)\n",
        "      gan_loss = common_layers.sliced_gan_loss(\n",
        "          target_codes,\n",
        "          rev_grad_gan_codes,\n",
        "          discriminate,\n",
        "          self.hparams.num_sliced_vecs,\n",
        "          do_tanh=hparams.sliced_do_tanh)\n",
        "      gan_loss *= hparams.gan_loss_factor * update_means_factor\n",
        "      losses[\"gan_loss\"] = -gan_loss\n",
        "\n",
        "    self.image_summary(\"ae\", reconstr)\n",
        "\n",
        "    logits = tf.reshape(reconstr, labels_shape + [vocab_size])\n",
        "    return logits, losses\n",
        "\n",
        "  def sample(self, features=None, shape=None):\n",
        "    del features\n",
        "    hp = self.hparams\n",
        "    div_x = 2**hp.num_hidden_layers\n",
        "    div_y = 1 if self.is1d else 2**hp.num_hidden_layers\n",
        "    size = [\n",
        "        hp.batch_size, hp.sample_height // div_x, hp.sample_width // div_y,\n",
        "        hp.bottleneck_bits\n",
        "    ]\n",
        "    size = size if shape is None else shape\n",
        "    # Sample in [-1, 1] as the bottleneck is under tanh.\n",
        "    return 2.0 * tf.random_uniform(size) - 1.0\n",
        "\n",
        "  def encode(self, x):\n",
        "    \"\"\"Auto-encode x and return the bottleneck.\"\"\"\n",
        "    features = {\"targets\": x}\n",
        "    self(features)  # pylint: disable=not-callable\n",
        "    res = tf.maximum(0.0, self._cur_bottleneck_tensor)  # Be 0/1 and not -1/1.\n",
        "    self._cur_bottleneck_tensor = None\n",
        "    return res\n",
        "\n",
        "  def infer(self, features, *args, **kwargs):  # pylint: disable=arguments-differ\n",
        "    \"\"\"Produce predictions from the model by sampling.\"\"\"\n",
        "    del args, kwargs\n",
        "    # Inputs and features preparation needed to handle edge cases.\n",
        "    if not features:\n",
        "      features = {}\n",
        "    inputs_old = None\n",
        "    if \"inputs\" in features and len(features[\"inputs\"].shape) < 4:\n",
        "      inputs_old = features[\"inputs\"]\n",
        "      features[\"inputs\"] = tf.expand_dims(features[\"inputs\"], 2)\n",
        "\n",
        "    # Sample and decode.\n",
        "    num_channels = self.num_channels\n",
        "    if \"targets\" not in features:\n",
        "      features[\"targets\"] = tf.zeros(\n",
        "          [self.hparams.batch_size, 1, 1, num_channels], dtype=tf.int32)\n",
        "    logits, _ = self(features)  # pylint: disable=not-callable\n",
        "    samples = tf.argmax(logits, axis=-1)\n",
        "\n",
        "    # Restore inputs to not confuse Estimator in edge cases.\n",
        "    if inputs_old is not None:\n",
        "      features[\"inputs\"] = inputs_old\n",
        "\n",
        "    # Return samples.\n",
        "    return samples\n",
        "\n",
        "  def decode(self, bottleneck):\n",
        "    \"\"\"Auto-decode from the bottleneck and return the result.\"\"\"\n",
        "    # Get the shape from bottleneck and num channels.\n",
        "    shape = common_layers.shape_list(bottleneck)\n",
        "    try:\n",
        "      num_channels = self.hparams.problem.num_channels\n",
        "    except AttributeError:\n",
        "      num_channels = 1\n",
        "    dummy_targets = tf.zeros(shape[:-1] + [num_channels])\n",
        "    # Set the bottleneck to decode.\n",
        "    if len(shape) > 4:\n",
        "      bottleneck = tf.squeeze(bottleneck, axis=[1])\n",
        "    bottleneck = 2 * bottleneck - 1  # Be -1/1 instead of 0/1.\n",
        "    self._cur_bottleneck_tensor = bottleneck\n",
        "    # Run decoding.\n",
        "    res = self.infer({\"targets\": dummy_targets})\n",
        "    self._cur_bottleneck_tensor = None\n",
        "    return res\n",
        "\n",
        "  def _get_kernel_and_strides(self):\n",
        "    hparams = self.hparams\n",
        "    kernel = (hparams.kernel_height, hparams.kernel_width)\n",
        "    kernel = (hparams.kernel_height, 1) if self.is1d else kernel\n",
        "    strides = (2, 1) if self.is1d else (2, 2)\n",
        "    return (kernel, strides)\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderAutoregressive(AutoencoderBasic):\n",
        "  \"\"\"Autoencoder with an autoregressive part.\"\"\"\n",
        "\n",
        "  def body(self, features):\n",
        "    hparams = self.hparams\n",
        "    # Run the basic autoencoder part first.\n",
        "    basic_result, losses = super(AutoencoderAutoregressive, self).body(features)\n",
        "    if hparams.autoregressive_mode == \"none\":\n",
        "      assert not hparams.autoregressive_forget_base\n",
        "      return basic_result, losses\n",
        "    if \"training\" in losses:\n",
        "      plain_training_loss = losses.pop(\"training\")\n",
        "      losses[\"plain\"] = plain_training_loss\n",
        "    res_shape = common_layers.shape_list(basic_result)\n",
        "    vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
        "    if hasattr(self._hparams, \"vocab_divisor\"):\n",
        "      vocab_size += (-vocab_size) % self._hparams.vocab_divisor\n",
        "    targets = tf.one_hot(features[\"targets_raw\"], vocab_size)\n",
        "    # Prepare inputs for autoregressive modes.\n",
        "    if common_layers.shape_list(features[\"targets\"])[1] == 1:\n",
        "      # This happens on the first step of predicitions.\n",
        "      assert hparams.mode == tf_estimator.ModeKeys.PREDICT\n",
        "      targets = tf.zeros_like(basic_result)\n",
        "    targets = self.embed(targets)\n",
        "    if hparams.autoregressive_gumbel_sample:\n",
        "      basic_hot = self.gumbel_sample(basic_result)\n",
        "    else:\n",
        "      basic_hot = basic_result\n",
        "    basic_result = self.embed(basic_hot)\n",
        "    shape = common_layers.shape_list(basic_result)\n",
        "    basic1d = tf.reshape(basic_result, [shape[0], -1, shape[-1]])\n",
        "    targets = tf.reshape(targets, common_layers.shape_list(basic_result))\n",
        "    # During autoregressive inference, don't resample.\n",
        "    if hparams.mode == tf_estimator.ModeKeys.PREDICT:\n",
        "      if hasattr(hparams, \"sampled_basic1d_tensor\"):\n",
        "        basic1d = hparams.sampled_basic1d_tensor\n",
        "      else:\n",
        "        hparams.sampled_basic1d_tensor = basic1d\n",
        "    # Sometimes it's useful to look at non-autoregressive evals.\n",
        "    targets_dropout = targets\n",
        "    if (hparams.mode == tf_estimator.ModeKeys.EVAL and\n",
        "        hparams.autoregressive_eval_pure_autoencoder):\n",
        "      targets_dropout = tf.zeros_like(basic_result)\n",
        "    # Now combine the basic reconstruction with shifted targets.\n",
        "    targets1d = tf.reshape(targets_dropout, [shape[0], -1, shape[-1]])\n",
        "    targets_shifted = common_layers.shift_right_3d(targets1d)\n",
        "    concat1d = tf.concat([basic1d, targets_shifted], axis=-1)\n",
        "    # The forget_base hparam sets purely-autoregressive mode, no autoencoder.\n",
        "    if hparams.autoregressive_forget_base:\n",
        "      concat1d = tf.reshape(targets, [shape[0], -1, shape[-1]])\n",
        "      concat1d = common_layers.shift_right_3d(concat1d)\n",
        "    # The autoregressive part depends on the mode.\n",
        "    if hparams.autoregressive_mode == \"conv3\":\n",
        "      res = common_layers.conv1d(\n",
        "          concat1d,\n",
        "          hparams.hidden_size,\n",
        "          3,\n",
        "          padding=\"LEFT\",\n",
        "          activation=common_layers.belu,\n",
        "          name=\"autoregressive_conv3\")\n",
        "      res = tf.layers.dense(res, vocab_size, name=\"autoregressive_final\")\n",
        "      return tf.reshape(res, res_shape), losses\n",
        "    if hparams.autoregressive_mode == \"conv5\":\n",
        "      res = common_layers.conv1d(\n",
        "          concat1d,\n",
        "          hparams.hidden_size,\n",
        "          5,\n",
        "          padding=\"LEFT\",\n",
        "          activation=common_layers.belu,\n",
        "          name=\"autoregressive_conv5\")\n",
        "      res = tf.layers.dense(res, vocab_size, name=\"autoregressive_final\")\n",
        "      return tf.reshape(res, res_shape), losses\n",
        "    if hparams.autoregressive_mode == \"sru\":\n",
        "      res = common_layers.conv1d(\n",
        "          concat1d,\n",
        "          hparams.hidden_size,\n",
        "          3,\n",
        "          padding=\"LEFT\",\n",
        "          activation=common_layers.belu,\n",
        "          name=\"autoregressive_sru_conv3\")\n",
        "      res = common_layers.sru(res)\n",
        "      res = tf.layers.dense(res, vocab_size, name=\"autoregressive_final\")\n",
        "      return tf.reshape(res, res_shape), losses\n",
        "\n",
        "    raise ValueError(\n",
        "        \"Unsupported autoregressive mode: %s\" % hparams.autoregressive_mode)\n",
        "\n",
        "  def infer(self, features, *args, **kwargs):\n",
        "    \"\"\"Produce predictions from the model by sampling.\"\"\"\n",
        "    # Inputs and features preparation needed to handle edge cases.\n",
        "    if not features:\n",
        "      features = {}\n",
        "    inputs_old = None\n",
        "    if \"inputs\" in features and len(features[\"inputs\"].shape) < 4:\n",
        "      inputs_old = features[\"inputs\"]\n",
        "      features[\"inputs\"] = tf.expand_dims(features[\"inputs\"], 2)\n",
        "\n",
        "    # Sample first.\n",
        "    try:\n",
        "      num_channels = self.hparams.problem.num_channels\n",
        "    except AttributeError:\n",
        "      num_channels = 1\n",
        "    if \"targets\" not in features:\n",
        "      features[\"targets\"] = tf.zeros(\n",
        "          [self.hparams.batch_size, 1, 1, num_channels], dtype=tf.int32)\n",
        "    logits, _ = self(features)  # pylint: disable=not-callable\n",
        "    samples = common_layers.sample_with_temperature(logits, 0.0)\n",
        "    shape = common_layers.shape_list(samples)\n",
        "\n",
        "    # Sample again if requested for the autoregressive part.\n",
        "    extra_samples = self.hparams.autoregressive_decode_steps\n",
        "    for i in range(extra_samples):\n",
        "      if i == extra_samples - 2:\n",
        "        self.hparams.sampling_temp /= 2\n",
        "      if i == extra_samples - 1:\n",
        "        self.hparams.sampling_temp = 0.0\n",
        "      features[\"targets\"] = samples\n",
        "      old_samples1d = tf.reshape(samples, [shape[0], -1, shape[3]])\n",
        "      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
        "        logits, _ = self(features)  # pylint: disable=not-callable\n",
        "        samples = common_layers.sample_with_temperature(\n",
        "            logits, self.hparams.sampling_temp)\n",
        "        samples1d = tf.reshape(samples, [shape[0], -1, shape[3]])\n",
        "        samples1d = tf.concat([old_samples1d[:, :i, :], samples1d[:, i:, :]],\n",
        "                              axis=1)\n",
        "        samples = tf.reshape(samples1d, shape)\n",
        "\n",
        "    # Restore inputs to not confuse Estimator in edge cases.\n",
        "    if inputs_old is not None:\n",
        "      features[\"inputs\"] = inputs_old\n",
        "\n",
        "    # Return samples.\n",
        "    return samples\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderResidual(AutoencoderAutoregressive):\n",
        "  \"\"\"Residual autoencoder.\"\"\"\n",
        "\n",
        "  def dropout(self, x):\n",
        "    is_training = self.hparams.mode == tf_estimator.ModeKeys.TRAIN\n",
        "    hparams = self.hparams\n",
        "    if hparams.dropout <= 0.0 or not is_training:\n",
        "      return x\n",
        "    warm_step = hparams.bottleneck_warmup_steps * 2**hparams.num_hidden_layers\n",
        "    dropout = common_layers.inverse_lin_decay(warm_step // 2) * hparams.dropout\n",
        "    return common_layers.dropout_with_broadcast_dims(\n",
        "        x, 1.0 - dropout, broadcast_dims=[-1])\n",
        "\n",
        "  def encoder(self, x):\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "      hparams = self.hparams\n",
        "      layers = []\n",
        "      kernel, strides = self._get_kernel_and_strides()\n",
        "      residual_kernel = (hparams.residual_kernel_height,\n",
        "                         hparams.residual_kernel_width)\n",
        "      residual_kernel1d = (hparams.residual_kernel_height, 1)\n",
        "      residual_kernel = residual_kernel1d if self.is1d else residual_kernel\n",
        "      residual_conv = tf.layers.conv2d\n",
        "      if hparams.residual_use_separable_conv:\n",
        "        residual_conv = tf.layers.separable_conv2d\n",
        "      # Down-convolutions.\n",
        "      for i in range(hparams.num_hidden_layers):\n",
        "        with tf.variable_scope(\"layer_%d\" % i):\n",
        "          x = self.make_even_size(x)\n",
        "          layers.append(x)\n",
        "          x = self.dropout(x)\n",
        "          filters = hparams.hidden_size * 2**(i + 1)\n",
        "          filters = min(filters, hparams.max_hidden_size)\n",
        "          x = common_attention.add_timing_signal_nd(x)\n",
        "          x = tf.layers.conv2d(\n",
        "              x,\n",
        "              filters,\n",
        "              kernel,\n",
        "              strides=strides,\n",
        "              padding=\"SAME\",\n",
        "              activation=common_layers.belu,\n",
        "              name=\"strided\")\n",
        "          y = x\n",
        "          y = tf.nn.dropout(y, 1.0 - hparams.residual_dropout)\n",
        "          for r in range(hparams.num_residual_layers):\n",
        "            residual_filters = filters\n",
        "            if r < hparams.num_residual_layers - 1:\n",
        "              residual_filters = int(\n",
        "                  filters * hparams.residual_filter_multiplier)\n",
        "            y = residual_conv(\n",
        "                y,\n",
        "                residual_filters,\n",
        "                residual_kernel,\n",
        "                padding=\"SAME\",\n",
        "                activation=common_layers.belu,\n",
        "                name=\"residual_%d\" % r)\n",
        "          x += y\n",
        "          x = common_layers.layer_norm(x, name=\"ln\")\n",
        "      return x, layers\n",
        "\n",
        "  def decoder(self, x, encoder_layers=None):\n",
        "    with tf.variable_scope(\"decoder\"):\n",
        "      hparams = self.hparams\n",
        "      is_training = self.hparams.mode == tf_estimator.ModeKeys.TRAIN\n",
        "      kernel, strides = self._get_kernel_and_strides()\n",
        "      residual_kernel = (hparams.residual_kernel_height,\n",
        "                         hparams.residual_kernel_width)\n",
        "      residual_kernel1d = (hparams.residual_kernel_height, 1)\n",
        "      residual_kernel = residual_kernel1d if self.is1d else residual_kernel\n",
        "      residual_conv = tf.layers.conv2d\n",
        "      if hparams.residual_use_separable_conv:\n",
        "        residual_conv = tf.layers.separable_conv2d\n",
        "      # Up-convolutions.\n",
        "      for i in range(hparams.num_hidden_layers):\n",
        "        j = hparams.num_hidden_layers - i - 1\n",
        "        if is_training:\n",
        "          nomix_p = common_layers.inverse_lin_decay(\n",
        "              int(hparams.bottleneck_warmup_steps * 0.25 * 2**j)) + 0.01\n",
        "          if common_layers.should_generate_summaries():\n",
        "            tf.summary.scalar(\"nomix_p_%d\" % j, nomix_p)\n",
        "        filters = hparams.hidden_size * 2**j\n",
        "        filters = min(filters, hparams.max_hidden_size)\n",
        "        with tf.variable_scope(\"layer_%d\" % i):\n",
        "          j = hparams.num_hidden_layers - i - 1\n",
        "          x = tf.layers.conv2d_transpose(\n",
        "              x,\n",
        "              filters,\n",
        "              kernel,\n",
        "              strides=strides,\n",
        "              padding=\"SAME\",\n",
        "              activation=common_layers.belu,\n",
        "              name=\"strided\")\n",
        "          y = x\n",
        "          for r in range(hparams.num_residual_layers):\n",
        "            residual_filters = filters\n",
        "            if r < hparams.num_residual_layers - 1:\n",
        "              residual_filters = int(\n",
        "                  filters * hparams.residual_filter_multiplier)\n",
        "            y = residual_conv(\n",
        "                y,\n",
        "                residual_filters,\n",
        "                residual_kernel,\n",
        "                padding=\"SAME\",\n",
        "                activation=common_layers.belu,\n",
        "                name=\"residual_%d\" % r)\n",
        "          x += tf.nn.dropout(y, 1.0 - hparams.residual_dropout)\n",
        "          x = common_layers.layer_norm(x, name=\"ln\")\n",
        "          x = common_attention.add_timing_signal_nd(x)\n",
        "          if encoder_layers is not None:\n",
        "            enc_x = encoder_layers[j]\n",
        "            enc_shape = common_layers.shape_list(enc_x)\n",
        "            x_mix = x[:enc_shape[0], :enc_shape[1], :enc_shape[2], :]\n",
        "            if is_training:  # Mix at the beginning of training.\n",
        "              rand = tf.random_uniform(common_layers.shape_list(x_mix))\n",
        "              x_mix = tf.where(tf.less(rand, nomix_p), x_mix, enc_x)\n",
        "            if hparams.gan_loss_factor != 0:\n",
        "              x_gan = x[enc_shape[0]:, :enc_shape[1], :enc_shape[2], :]\n",
        "              x = tf.concat([x_mix, x_gan], axis=0)\n",
        "            else:\n",
        "              x = x_mix\n",
        "      return x\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderResidualVAE(AutoencoderResidual):\n",
        "  \"\"\"Residual VAE autoencoder.\"\"\"\n",
        "\n",
        "  def bottleneck(self, x):\n",
        "    hparams = self.hparams\n",
        "    z_size = hparams.bottleneck_bits\n",
        "    x_shape = common_layers.shape_list(x)\n",
        "    with tf.variable_scope(\"vae\"):\n",
        "      mu = tf.layers.dense(x, z_size, name=\"mu\")\n",
        "      if hparams.mode != tf_estimator.ModeKeys.TRAIN:\n",
        "        return mu, 0.0  # No sampling or kl loss on eval.\n",
        "      log_sigma = tf.layers.dense(x, z_size, name=\"log_sigma\")\n",
        "      epsilon = tf.random_normal(x_shape[:-1] + [z_size])\n",
        "      z = mu + tf.exp(log_sigma / 2) * epsilon\n",
        "      kl = 0.5 * tf.reduce_mean(\n",
        "          tf.expm1(log_sigma) + tf.square(mu) - log_sigma, axis=-1)\n",
        "      free_bits = z_size // 4\n",
        "      kl_loss = tf.reduce_mean(tf.maximum(kl - free_bits, 0.0))\n",
        "    return z, kl_loss * hparams.kl_beta\n",
        "\n",
        "  def sample(self, features=None, shape=None):\n",
        "    del features\n",
        "    hparams = self.hparams\n",
        "    div_x = 2**hparams.num_hidden_layers\n",
        "    div_y = 1 if self.is1d else 2**hparams.num_hidden_layers\n",
        "    size = [\n",
        "        hparams.batch_size, hparams.sample_height // div_x,\n",
        "        hparams.sample_width // div_y, hparams.bottleneck_bits\n",
        "    ]\n",
        "    size = size if shape is None else shape\n",
        "    return tf.random_normal(size)\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderBasicDiscrete(AutoencoderAutoregressive):\n",
        "  \"\"\"Discrete autoencoder.\"\"\"\n",
        "\n",
        "  def bottleneck(self, x):\n",
        "    hparams = self.hparams\n",
        "    x = tf.tanh(tf.layers.dense(x, hparams.bottleneck_bits, name=\"bottleneck\"))\n",
        "    d = x + tf.stop_gradient(2.0 * tf.to_float(tf.less(0.0, x)) - 1.0 - x)\n",
        "    if hparams.mode == tf_estimator.ModeKeys.TRAIN:\n",
        "      noise = tf.random_uniform(common_layers.shape_list(x))\n",
        "      noise = 2.0 * tf.to_float(tf.less(hparams.bottleneck_noise, noise)) - 1.0\n",
        "      d *= noise\n",
        "    x = common_layers.mix(d, x, hparams.discretize_warmup_steps,\n",
        "                          hparams.mode == tf_estimator.ModeKeys.TRAIN)\n",
        "    return x, 0.0\n",
        "\n",
        "  def sample(self, features=None, shape=None):\n",
        "    del features\n",
        "    hp = self.hparams\n",
        "    div_x = 2**hp.num_hidden_layers\n",
        "    div_y = 1 if self.is1d else 2**hp.num_hidden_layers\n",
        "    size = [\n",
        "        hp.batch_size, hp.sample_height // div_x, hp.sample_width // div_y,\n",
        "        hp.bottleneck_bits\n",
        "    ]\n",
        "    size = size if shape is None else shape\n",
        "    rand = tf.random_uniform(size)\n",
        "    return 2.0 * tf.to_float(tf.less(0.5, rand)) - 1.0\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderResidualDiscrete(AutoencoderResidual):\n",
        "  \"\"\"Discrete residual autoencoder.\"\"\"\n",
        "\n",
        "  def variance_loss(self, b):\n",
        "    part = tf.random_uniform(common_layers.shape_list(b))\n",
        "    selection = tf.to_float(tf.less(part, tf.random_uniform([])))\n",
        "    selection_size = tf.reduce_sum(selection)\n",
        "    part_avg = tf.abs(tf.reduce_sum(b * selection)) / (selection_size + 1)\n",
        "    return part_avg\n",
        "\n",
        "  def bottleneck(self, x, bottleneck_bits=None):  # pylint: disable=arguments-differ\n",
        "    if bottleneck_bits is not None:\n",
        "      old_bottleneck_bits = self.hparams.bottleneck_bits\n",
        "      self.hparams.bottleneck_bits = bottleneck_bits\n",
        "    res, loss = discretization.parametrized_bottleneck(x, self.hparams)\n",
        "    if bottleneck_bits is not None:\n",
        "      self.hparams.bottleneck_bits = old_bottleneck_bits\n",
        "    return res, loss\n",
        "\n",
        "  def unbottleneck(self, x, res_size, reuse=None):\n",
        "    with tf.variable_scope(\"unbottleneck\", reuse=reuse):\n",
        "      return discretization.parametrized_unbottleneck(x, res_size, self.hparams)\n",
        "\n",
        "  def sample(self, features=None, shape=None):\n",
        "    del features\n",
        "    hp = self.hparams\n",
        "    div_x = 2**hp.num_hidden_layers\n",
        "    div_y = 1 if self.is1d else 2**hp.num_hidden_layers\n",
        "    size = [\n",
        "        hp.batch_size, hp.sample_height // div_x, hp.sample_width // div_y,\n",
        "        hp.bottleneck_bits\n",
        "    ]\n",
        "    size = size if shape is None else shape\n",
        "    rand = tf.random_uniform(size)\n",
        "    res = 2.0 * tf.to_float(tf.less(0.5, rand)) - 1.0\n",
        "    # If you want to set some first bits to a fixed value, do this:\n",
        "    # fixed = tf.zeros_like(rand) - 1.0\n",
        "    # nbits = 3\n",
        "    # res = tf.concat([fixed[:, :, :, :nbits], res[:, :, :, nbits:]], axis=-1)\n",
        "    return res\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderOrderedDiscrete(AutoencoderResidualDiscrete):\n",
        "  \"\"\"Ordered discrete autoencoder.\"\"\"\n",
        "\n",
        "  def bottleneck(self, x):  # pylint: disable=arguments-differ\n",
        "    hparams = self.hparams\n",
        "    if hparams.unordered:\n",
        "      return super(AutoencoderOrderedDiscrete, self).bottleneck(x)\n",
        "    noise = hparams.bottleneck_noise\n",
        "    hparams.bottleneck_noise = 0.0  # We'll add noise below.\n",
        "    x, loss = discretization.parametrized_bottleneck(x, hparams)\n",
        "    hparams.bottleneck_noise = noise\n",
        "    if hparams.mode == tf_estimator.ModeKeys.TRAIN:\n",
        "      # We want a number p such that p^bottleneck_bits = 1 - noise.\n",
        "      # So log(p) * bottleneck_bits = log(noise)\n",
        "      log_p = tf.log1p(-float(noise) / 2) / float(hparams.bottleneck_bits)\n",
        "      # Probabilities of flipping are p, p^2, p^3, ..., p^bottleneck_bits.\n",
        "      noise_mask = 1.0 - tf.exp(tf.cumsum(tf.zeros_like(x) + log_p, axis=-1))\n",
        "      # Having the no-noise mask, we can make noise just uniformly at random.\n",
        "      ordered_noise = tf.random_uniform(tf.shape(x))\n",
        "      # We want our noise to be 1s at the start and random {-1, 1} bits later.\n",
        "      ordered_noise = tf.to_float(tf.less(noise_mask, ordered_noise))\n",
        "      # Now we flip the bits of x on the noisy positions (ordered and normal).\n",
        "      x *= 2.0 * ordered_noise - 1\n",
        "    return x, loss\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderDualDiscrete(AutoencoderResidualDiscrete):\n",
        "  \"\"\"Dual discrete autoencoder.\"\"\"\n",
        "\n",
        "  def body(self, features):\n",
        "    if self.hparams.mode != tf_estimator.ModeKeys.EVAL:\n",
        "      t, i = features[\"targets_raw\"], features[\"inputs_raw\"]\n",
        "      t, i = common_layers.pad_to_same_length(t, i)\n",
        "      features[\"targets_raw\"] = tf.concat([t, i], axis=0)\n",
        "    return super(AutoencoderDualDiscrete, self).body(features)\n",
        "\n",
        "  def embed(self, x, name=\"embedding\"):\n",
        "    if self.hparams.mode == tf_estimator.ModeKeys.EVAL:\n",
        "      return super(AutoencoderDualDiscrete, self).embed(x, name=name + \"_t\")\n",
        "    xt, xi = tf.split(x, 2, axis=0)\n",
        "    xte = super(AutoencoderDualDiscrete, self).embed(xt, name=name + \"_t\")\n",
        "    xie = super(AutoencoderDualDiscrete, self).embed(xi, name=name + \"_i\")\n",
        "    return tf.concat([xte, xie], axis=0)\n",
        "\n",
        "  def bottleneck(self, x):\n",
        "    hparams = self.hparams\n",
        "    b, _ = super(AutoencoderDualDiscrete, self).bottleneck(x)\n",
        "    if hparams.mode == tf_estimator.ModeKeys.EVAL:\n",
        "      return b, 0.0\n",
        "    bt, bi = tf.split(b, 2, axis=0)\n",
        "    if self.hparams.mode != tf_estimator.ModeKeys.TRAIN:\n",
        "      return tf.concat([bi, bi], axis=0), 0.0\n",
        "    # Share the first hparams.bottleneck_shared_bits.\n",
        "    shared = (bt + bi) / 2  # -1 if both -1, 1 if both were 1, 0 if disagree.\n",
        "    rand = tf.random_uniform(common_layers.shape_list(bt))\n",
        "    br = tf.where(rand < 0.5, bt, bi)  # Break ties at random.\n",
        "    bs = tf.where(shared == 0, br, shared)\n",
        "    bs = tf.concat([bs, bs], axis=0)\n",
        "    n = hparams.bottleneck_shared_bits\n",
        "    step = tf.train.get_global_step()\n",
        "    zero = tf.constant(0, dtype=tf.int64)\n",
        "    if step is None:\n",
        "      step = zero\n",
        "    step = tf.maximum(zero, step - hparams.bottleneck_shared_bits_start_warmup)\n",
        "    f = common_layers.inverse_lin_decay(\n",
        "        hparams.bottleneck_shared_bits_stop_warmup, min_value=0.1, step=step)\n",
        "    n = tf.where(step > 1, n * f, n)\n",
        "    n = tf.cast(n, tf.int64)\n",
        "    b_shape = common_layers.shape_list(b)\n",
        "    b = tf.concat([bs[..., :n], b[..., n:]], axis=-1)\n",
        "    b = tf.reshape(b, b_shape)\n",
        "    return b, 0.0\n",
        "\n",
        "  def unbottleneck(self, b, res_size, reuse=None):\n",
        "    x = super(AutoencoderDualDiscrete, self).unbottleneck(\n",
        "        b, res_size, reuse=reuse)\n",
        "    if self.hparams.mode == tf_estimator.ModeKeys.EVAL:\n",
        "      return tf.layers.dense(x, res_size, name=\"dual_unbottleneck_t\")\n",
        "    xt, xi = tf.split(x, 2, axis=0)\n",
        "    xt = tf.layers.dense(xt, res_size, name=\"dual_unbottleneck_t\")\n",
        "    xi = tf.layers.dense(xt, res_size, name=\"dual_unbottleneck_i\")\n",
        "    return tf.concat([xt, xi], axis=0)\n",
        "\n",
        "  def infer(self, features, *args, **kwargs):  # pylint: disable=arguments-differ\n",
        "    \"\"\"Produce predictions from the model.\"\"\"\n",
        "    del args, kwargs\n",
        "    # Inputs and features preparation needed to handle edge cases.\n",
        "    if not features:\n",
        "      features = {}\n",
        "    inputs_old = None\n",
        "    if \"inputs\" in features and len(features[\"inputs\"].shape) < 4:\n",
        "      inputs_old = features[\"inputs\"]\n",
        "      features[\"inputs\"] = tf.expand_dims(features[\"inputs\"], 2)\n",
        "\n",
        "    # Set targets to input size firts.\n",
        "    features[\"targets\"] = tf.zeros_like(features[\"inputs\"])\n",
        "    self._encode_on_predict = True\n",
        "    logits, _ = self(features)  # pylint: disable=not-callable\n",
        "    if self.hparams.gan_loss_factor != 0:\n",
        "      logits, _ = tf.split(logits, 2, axis=0)  # Remove GAN.\n",
        "    logits, _ = tf.split(logits, 2, axis=0)  # Targets and inputs from encoding.\n",
        "    # Uncomment the line below to get reconstructed inputs instead of targets.\n",
        "    # (and comment out the line above at the same time).\n",
        "    # _, logits = tf.split(logits, 2, axis=0)\n",
        "    samples = tf.argmax(logits, axis=-1)\n",
        "\n",
        "    # Restore inputs to not confuse Estimator in edge cases.\n",
        "    if inputs_old is not None:\n",
        "      features[\"inputs\"] = inputs_old\n",
        "\n",
        "    # Return samples.\n",
        "    return samples\n",
        "\n",
        "\n",
        "@registry.register_model\n",
        "class AutoencoderStacked(AutoencoderResidualDiscrete):\n",
        "  \"\"\"A stacked autoencoder.\"\"\"\n",
        "\n",
        "  def stack(self, b, size, bottleneck_bits, name):\n",
        "    with tf.variable_scope(name + \"_stack\"):\n",
        "      unb = self.unbottleneck(b, size)\n",
        "      enc = self.encoder(unb)\n",
        "      b, _ = self.bottleneck(enc, bottleneck_bits=bottleneck_bits)\n",
        "      return b\n",
        "\n",
        "  def unstack(self, b, size, bottleneck_bits, name):\n",
        "    with tf.variable_scope(name + \"_unstack\"):\n",
        "      unb = self.unbottleneck(b, size)\n",
        "      dec = self.decoder(unb)\n",
        "      pred = tf.layers.dense(dec, bottleneck_bits, name=\"pred\")\n",
        "      pred_shape = common_layers.shape_list(pred)\n",
        "      pred1 = tf.reshape(pred, pred_shape[:-1] + [-1, 2])\n",
        "      x, y = tf.split(pred1, 2, axis=-1)\n",
        "      x = tf.squeeze(x, axis=[-1])\n",
        "      y = tf.squeeze(y, axis=[-1])\n",
        "      gt = 2.0 * tf.to_float(tf.less(x, y)) - 1.0\n",
        "      gtc = tf.tanh(y - x)\n",
        "      gt += gtc - tf.stop_gradient(gtc)\n",
        "      return gt, pred1\n",
        "\n",
        "  def stack_loss(self, b, b_pred, name):\n",
        "    with tf.variable_scope(name):\n",
        "      labels_discrete = tf.to_int32((b + 1.0) * 0.5)\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          labels=labels_discrete, logits=b_pred)\n",
        "      return tf.reduce_mean(loss)\n",
        "\n",
        "  def full_stack(self, b, x_size, bottleneck_bits, losses, is_training, i):\n",
        "    stack1_b = self.stack(b, x_size, bottleneck_bits, \"step%d\" % i)\n",
        "    if i > 1:\n",
        "      stack1_b = self.full_stack(stack1_b, 2 * x_size, 2 * bottleneck_bits,\n",
        "                                 losses, is_training, i - 1)\n",
        "    b1, b_pred = self.unstack(stack1_b, x_size, bottleneck_bits, \"step%d\" % i)\n",
        "    losses[\"stack%d_loss\" % i] = self.stack_loss(b, b_pred, \"step%d\" % i)\n",
        "    b_shape = common_layers.shape_list(b)\n",
        "    if is_training:\n",
        "      condition = tf.less(tf.random_uniform([]), 0.5)\n",
        "      condition = tf.reshape(condition, [1] * len(b.shape))\n",
        "      condition = tf.tile(condition, b.shape)\n",
        "      b1 = tf.where(condition, b, b1)\n",
        "    return tf.reshape(b1, b_shape)\n",
        "\n",
        "  def body(self, features):\n",
        "    hparams = self.hparams\n",
        "    num_stacks = hparams.num_hidden_layers\n",
        "    hparams.num_hidden_layers = 1\n",
        "    is_training = hparams.mode == tf_estimator.ModeKeys.TRAIN\n",
        "    if hparams.mode != tf_estimator.ModeKeys.PREDICT:\n",
        "      x = features[\"targets\"]\n",
        "      shape = common_layers.shape_list(x)\n",
        "      is1d = shape[2] == 1\n",
        "      self.is1d = is1d\n",
        "      x, _ = common_layers.pad_to_same_length(\n",
        "          x, x, final_length_divisible_by=2**num_stacks, axis=1)\n",
        "      if not is1d:\n",
        "        x, _ = common_layers.pad_to_same_length(\n",
        "            x, x, final_length_divisible_by=2**num_stacks, axis=2)\n",
        "      # Run encoder.\n",
        "      x = self.encoder(x)\n",
        "      x_size = common_layers.shape_list(x)[-1]\n",
        "      # Bottleneck (mix during early training, not too important but stable).\n",
        "      b, b_loss = self.bottleneck(x)\n",
        "      losses = {\"bottleneck0_loss\": b_loss}\n",
        "      b = self.full_stack(b, 2 * x_size, 2 * hparams.bottleneck_bits, losses,\n",
        "                          is_training, num_stacks - 1)\n",
        "      b = self.unbottleneck(b, x_size)\n",
        "      b = common_layers.mix(b, x, hparams.bottleneck_warmup_steps, is_training)\n",
        "      x = b\n",
        "    else:\n",
        "      b = self.sample()\n",
        "      res_size = self.hparams.hidden_size * 2**self.hparams.num_hidden_layers\n",
        "      res_size = min(res_size, hparams.max_hidden_size)\n",
        "      x = self.unbottleneck(b, res_size)\n",
        "    # Run decoder.\n",
        "    x = self.decoder(x)\n",
        "    if hparams.mode == tf_estimator.ModeKeys.PREDICT:\n",
        "      return x\n",
        "    # Cut to the right size and mix before returning.\n",
        "    res = x[:, :shape[1], :shape[2], :]\n",
        "    res = common_layers.mix(res, features[\"targets\"],\n",
        "                            hparams.bottleneck_warmup_steps // 2, is_training)\n",
        "    hparams.num_hidden_layers = num_stacks\n",
        "    return res, losses\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_basic():\n",
        "  \"\"\"Basic autoencoder model.\"\"\"\n",
        "  hparams = common_hparams.basic_params1()\n",
        "  hparams.optimizer = \"adam\"\n",
        "  hparams.learning_rate_constant = 0.0002\n",
        "  hparams.learning_rate_warmup_steps = 500\n",
        "  hparams.learning_rate_schedule = \"constant * linear_warmup\"\n",
        "  hparams.label_smoothing = 0.0\n",
        "  hparams.batch_size = 128\n",
        "  hparams.hidden_size = 64\n",
        "  hparams.num_hidden_layers = 5\n",
        "  hparams.initializer = \"uniform_unit_scaling\"\n",
        "  hparams.initializer_gain = 1.0\n",
        "  hparams.weight_decay = 0.0\n",
        "  hparams.kernel_height = 4\n",
        "  hparams.kernel_width = 4\n",
        "  hparams.dropout = 0.05\n",
        "  hparams.add_hparam(\"max_hidden_size\", 1024)\n",
        "  hparams.add_hparam(\"bottleneck_bits\", 128)\n",
        "  hparams.add_hparam(\"bottleneck_shared_bits\", 0)\n",
        "  hparams.add_hparam(\"bottleneck_shared_bits_start_warmup\", 0)\n",
        "  hparams.add_hparam(\"bottleneck_shared_bits_stop_warmup\", 0)\n",
        "  hparams.add_hparam(\"bottleneck_noise\", 0.1)\n",
        "  hparams.add_hparam(\"bottleneck_warmup_steps\", 2000)\n",
        "  hparams.add_hparam(\"sample_height\", 32)\n",
        "  hparams.add_hparam(\"sample_width\", 32)\n",
        "  hparams.add_hparam(\"discriminator_batchnorm\", True)\n",
        "  hparams.add_hparam(\"num_sliced_vecs\", 20000)\n",
        "  hparams.add_hparam(\"sliced_do_tanh\", int(True))\n",
        "  hparams.add_hparam(\"discriminator_size\", 256)\n",
        "  hparams.add_hparam(\"discriminator_kernel_size\", 6)\n",
        "  hparams.add_hparam(\"discriminator_strides\", 4)\n",
        "  hparams.add_hparam(\"discriminator_pure_mean\", int(False))\n",
        "  hparams.add_hparam(\"code_loss_factor\", 1.0)\n",
        "  hparams.add_hparam(\"gan_codes_warmup_steps\", 16000)\n",
        "  hparams.add_hparam(\"gan_loss_factor\", 0.0)\n",
        "  hparams.add_hparam(\"bottleneck_l2_factor\", 0.05)\n",
        "  hparams.add_hparam(\"gumbel_temperature\", 0.5)\n",
        "  hparams.add_hparam(\"gumbel_noise_factor\", 0.5)\n",
        "  hparams.add_hparam(\"vq_temperature\", 0.001)\n",
        "  hparams.add_hparam(\"use_vq_loss\", int(False))\n",
        "  hparams.add_hparam(\"discriminator\", \"double\")\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_autoregressive():\n",
        "  \"\"\"Autoregressive autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_basic()\n",
        "  hparams.add_hparam(\"autoregressive_forget_base\", False)\n",
        "  hparams.add_hparam(\"autoregressive_mode\", \"none\")\n",
        "  hparams.add_hparam(\"autoregressive_decode_steps\", 0)\n",
        "  hparams.add_hparam(\"autoregressive_eval_pure_autoencoder\", False)\n",
        "  hparams.add_hparam(\"autoregressive_gumbel_sample\", False)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_residual():\n",
        "  \"\"\"Residual autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_autoregressive()\n",
        "  hparams.optimizer = \"Adafactor\"\n",
        "  hparams.clip_grad_norm = 1.0\n",
        "  hparams.learning_rate_constant = 0.5\n",
        "  hparams.learning_rate_warmup_steps = 500\n",
        "  hparams.learning_rate_schedule = \"constant * linear_warmup * rsqrt_decay\"\n",
        "  hparams.num_hidden_layers = 5\n",
        "  hparams.hidden_size = 64\n",
        "  hparams.max_hidden_size = 1024\n",
        "  hparams.add_hparam(\"num_residual_layers\", 2)\n",
        "  hparams.add_hparam(\"residual_kernel_height\", 3)\n",
        "  hparams.add_hparam(\"residual_kernel_width\", 3)\n",
        "  hparams.add_hparam(\"residual_filter_multiplier\", 2.0)\n",
        "  hparams.add_hparam(\"residual_dropout\", 0.2)\n",
        "  hparams.add_hparam(\"residual_use_separable_conv\", int(True))\n",
        "  hparams.add_hparam(\"kl_beta\", 1.0)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_residual_text():\n",
        "  \"\"\"Residual autoencoder model for text.\"\"\"\n",
        "  hparams = autoencoder_residual()\n",
        "  hparams.bottleneck_bits = 32\n",
        "  hparams.batch_size = 1024\n",
        "  hparams.hidden_size = 64\n",
        "  hparams.max_hidden_size = 512\n",
        "  hparams.bottleneck_noise = 0.0\n",
        "  hparams.bottom = {\n",
        "      \"inputs\": modalities.identity_bottom,\n",
        "      \"targets\": modalities.identity_bottom,\n",
        "  }\n",
        "  hparams.top = {\n",
        "      \"targets\": modalities.identity_top,\n",
        "  }\n",
        "  hparams.autoregressive_mode = \"none\"\n",
        "  hparams.sample_width = 1\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_basic_discrete():\n",
        "  \"\"\"Basic autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_autoregressive()\n",
        "  hparams.num_hidden_layers = 5\n",
        "  hparams.hidden_size = 64\n",
        "  hparams.bottleneck_bits = 1024\n",
        "  hparams.bottleneck_noise = 0.1\n",
        "  hparams.add_hparam(\"discretize_warmup_steps\", 16000)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_residual_discrete():\n",
        "  \"\"\"Residual discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_residual()\n",
        "  hparams.bottleneck_bits = 1024\n",
        "  hparams.bottleneck_noise = 0.05\n",
        "  hparams.add_hparam(\"discretize_warmup_steps\", 16000)\n",
        "  hparams.add_hparam(\"bottleneck_kind\", \"tanh_discrete\")\n",
        "  hparams.add_hparam(\"isemhash_noise_dev\", 0.5)\n",
        "  hparams.add_hparam(\"isemhash_mix_prob\", 0.5)\n",
        "  hparams.add_hparam(\"isemhash_filter_size_multiplier\", 2.0)\n",
        "  hparams.add_hparam(\"vq_beta\", 0.25)\n",
        "  hparams.add_hparam(\"vq_decay\", 0.999)\n",
        "  hparams.add_hparam(\"vq_epsilon\", 1e-5)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_residual_discrete_big():\n",
        "  \"\"\"Residual discrete autoencoder model, big version.\"\"\"\n",
        "  hparams = autoencoder_residual_discrete()\n",
        "  hparams.hidden_size = 128\n",
        "  hparams.max_hidden_size = 4096\n",
        "  hparams.bottleneck_noise = 0.1\n",
        "  hparams.residual_dropout = 0.4\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete():\n",
        "  \"\"\"Ordered discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_residual_discrete()\n",
        "  hparams.bottleneck_noise = 0.05  # Use 0.8 for ordered.\n",
        "  hparams.gan_loss_factor = 0.05\n",
        "  hparams.add_hparam(\"unordered\", True)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete_image64():\n",
        "  \"\"\"Ordered discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.batch_size = 32\n",
        "  hparams.num_hidden_layers = 6\n",
        "  hparams.bottleneck_warmup_steps *= 2\n",
        "  hparams.gan_codes_warmup_steps *= 2\n",
        "\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete_patched():\n",
        "  \"\"\"Ordered discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.discriminator = \"patched\"\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete_single():\n",
        "  \"\"\"Ordered discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.discriminator = \"single\"\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete_hs256():\n",
        "  \"\"\"Ordered discrete autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.hidden_size = 256\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_text():\n",
        "  \"\"\"Ordered discrete autoencoder model for text.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.bottleneck_bits = 1024\n",
        "  hparams.bottleneck_shared_bits = 1024-64\n",
        "  hparams.bottleneck_shared_bits_start_warmup = 75000\n",
        "  hparams.bottleneck_shared_bits_stop_warmup = 275000\n",
        "  hparams.num_hidden_layers = 7\n",
        "  hparams.batch_size = 1024\n",
        "  hparams.autoregressive_mode = \"conv5\"\n",
        "  hparams.max_hidden_size = 1024\n",
        "  hparams.bottom = {\n",
        "      \"inputs\": modalities.identity_bottom,\n",
        "      \"targets\": modalities.identity_bottom,\n",
        "  }\n",
        "  hparams.top = {\n",
        "      \"targets\": modalities.identity_top,\n",
        "  }\n",
        "  hparams.sample_height = 128\n",
        "  hparams.sample_width = 1\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_text_small():\n",
        "  \"\"\"Ordered discrete autoencoder model for text, small version.\"\"\"\n",
        "  hparams = autoencoder_ordered_text()\n",
        "  hparams.bottleneck_bits = 32\n",
        "  hparams.num_hidden_layers = 3\n",
        "  hparams.hidden_size = 64\n",
        "  hparams.max_hidden_size = 512\n",
        "  hparams.bottleneck_noise = 0.0\n",
        "  hparams.autoregressive_mode = \"conv5\"\n",
        "  hparams.sample_height = 4\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_ordered_discrete_vq():\n",
        "  \"\"\"Ordered discrete autoencoder model with VQ bottleneck.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.bottleneck_kind = \"vq\"\n",
        "  hparams.bottleneck_bits = 16\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_discrete_pong():\n",
        "  \"\"\"Discrete autoencoder model for compressing pong frames.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.num_hidden_layers = 3\n",
        "  hparams.bottleneck_bits = 24\n",
        "  hparams.batch_size = 2\n",
        "  hparams.gan_loss_factor = 0.01\n",
        "  hparams.bottleneck_l2_factor = 0.001\n",
        "  hparams.add_hparam(\"video_modality_loss_cutoff\", 0.02)\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_discrete_tiny():\n",
        "  \"\"\"Discrete autoencoder model for compressing pong frames for testing.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.num_hidden_layers = 2\n",
        "  hparams.bottleneck_bits = 24\n",
        "  hparams.batch_size = 2\n",
        "  hparams.gan_loss_factor = 0.\n",
        "  hparams.bottleneck_l2_factor = 0.001\n",
        "  hparams.add_hparam(\"video_modality_loss_cutoff\", 0.02)\n",
        "  hparams.num_residual_layers = 1\n",
        "  hparams.hidden_size = 32\n",
        "  hparams.max_hidden_size = 64\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_discrete_cifar():\n",
        "  \"\"\"Discrete autoencoder model for compressing cifar.\"\"\"\n",
        "  hparams = autoencoder_ordered_discrete()\n",
        "  hparams.bottleneck_noise = 0.0\n",
        "  hparams.bottleneck_bits = 90\n",
        "  hparams.num_hidden_layers = 2\n",
        "  hparams.hidden_size = 256\n",
        "  hparams.num_residual_layers = 4\n",
        "  hparams.batch_size = 32\n",
        "  hparams.learning_rate_constant = 1.0\n",
        "  return hparams\n",
        "\n",
        "\n",
        "@registry.register_ranged_hparams\n",
        "def autoencoder_range(rhp):\n",
        "  \"\"\"Tuning grid of the main autoencoder params.\"\"\"\n",
        "  rhp.set_float(\"dropout\", 0.01, 0.3)\n",
        "  rhp.set_float(\"gan_loss_factor\", 0.01, 0.1)\n",
        "  rhp.set_float(\"bottleneck_l2_factor\", 0.001, 0.1, scale=rhp.LOG_SCALE)\n",
        "  rhp.set_discrete(\"bottleneck_warmup_steps\", [200, 2000])\n",
        "  rhp.set_float(\"gumbel_temperature\", 0, 1)\n",
        "  rhp.set_float(\"gumbel_noise_factor\", 0, 0.5)\n",
        "\n",
        "\n",
        "@registry.register_ranged_hparams\n",
        "def autoencoder_discrete_pong_range(rhp):\n",
        "  \"\"\"Narrow tuning grid.\"\"\"\n",
        "  rhp.set_float(\"dropout\", 0.0, 0.2)\n",
        "  rhp.set_discrete(\"max_hidden_size\", [1024, 2048])\n",
        "\n",
        "\n",
        "@registry.register_hparams\n",
        "def autoencoder_stacked():\n",
        "  \"\"\"Stacked autoencoder model.\"\"\"\n",
        "  hparams = autoencoder_residual_discrete()\n",
        "  hparams.bottleneck_bits = 128\n",
        "  return hparams"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2hD06ETgDek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}